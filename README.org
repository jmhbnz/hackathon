#+TITLE: OpenShift AI Hackathon
#+AUTHOR: James Blair, Tom Corcoran, Neo Xu
#+DATE: <2024-06-06 Thu>

This repository contains a basic [[https://nextjs.org/][nextjs]] frontend designed to be exported as a static site and served via [[https://pages.github.com/][github pages]], for the purposes of running an OpenShift AI hackathon.

Below are the instructions for manually setting up an environment to run the hackathon.

* Cluster provisioning

Each team participating in the hackathon will require a [[https://aws.amazon.com/rosa][Red Hat OpenShift on AWS (ROSA)]] cluster, which we will provision via the [[https://demo.redhat.com/catalog?item=babylon-catalog-prod/sandboxes-gpte.rosa.prod&utm_source=webapp&utm_medium=share-link][Red Hat Demo System]]. When requesting the environments we enable the workshop user interface with:
- The title `OpenShift AI Hackathon`
- Number of instances set to `12`


* Cluster setup

For each cluster provisioned for the hackathon, the following steps need to be performed:


** Log in to cluster

#+begin_src tmux
oc login --web <api-route>
#+end_src

** Create gpu machine pool

Our first task is to ensure each cluster has a GPU `MachineSet` present, we can follow the instructions from https://cloud.redhat.com/experts/rosa/gpu to complete this.

#+begin_src tmux
# Define paramaters for machineset
export GPU_INSTANCE_TYPE='g5.8xlarge'
export CLUSTER_NAME=<cluster-name>
export MACHINE_POOL_NAME=nvidia-gpu-pool
export MACHINE_POOL_REPLICA_COUNT=1

# Create the machineset with rosa cli
rosa create machinepool \
  --cluster="${CLUSTER_NAME}" \
  --name="${MACHINE_POOL_NAME}" \
  --replicas="${MACHINE_POOL_REPLICA_COUNT}" \
  --instance-type="${GPU_INSTANCE_TYPE}"

# Wait for the machineset to be ready
oc wait --for=jsonpath='{.status.readyReplicas}'=1 machineset \
  --selector hive.openshift.io/machine-pool="${MACHINE_POOL_NAME}" \
  --namespace openshift-machine-api \
  --timeout=600s
#+end_src


** Install minio via oc

#+begin_src tmux
oc new-project minio

oc apply -f setup/minio-setup.yaml

oc rollout status deployment/minio --watch


#+end_src

** TODO Consider creating a cluster web terminal pod

** TODO Create Minio Bucket `models`

** Download model from huggingface into each `on prem` clusters Minio `model`'s bucket

#+begin_src tmux
HUGGINGFACE_TOKEN="HUGGINGFACE_TOKEN"
pip install --upgrade huggingface_hub
huggingface-cli login --token "${HUGGINGFACE_TOKEN}"
git clone https://huggingface.co/instructlab/granite-7b-lab
#+end_src

or use this:
https://github.com/tnscorcoran/rhods-finetunning-demo/blob/main/vllm_get_from_huggingface.ipynb


** Upload model to cluster minio
Consider using this:
https://github.com/tnscorcoran/rhods-finetunning-demo/blob/main/vllm_push_to_minio.ipynb

TODO Run aws configure and pull values out of that automatically.

#+begin_src tmux
export AWS_ACCESS_KEY_ID=<placeholder>
export AWS_SECRET_ACCESS_KEY=<placeholder>
export AWS_DEFAULT_REGION=<placeholder>
export AWS_S3_ENDPOINT=$(oc get route minio-api -o jsonpath='.spec.host')
export AWS_S3_BUCKET="models"

python3 setup/minio-upload.py
#+end_src


